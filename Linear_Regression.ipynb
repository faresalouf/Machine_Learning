{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "organic-vermont",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "## 1. <u>Basic equation</u>\n",
    "$\\hat{y} = \\theta_0 + \\theta_1x_1 + ... + \\theta_nx_n$    \n",
    "     \n",
    "In this equation:\n",
    "   * $\\hat{y}$ is the predicted value.\n",
    "   * $n$ is the number of features.\n",
    "   * $x_i$ is the $i^{th}$ feature.\n",
    "   * $\\theta_j$ is the $j^{th}$ model parameter.\n",
    "   * $\\theta_0$ is the bias term.\n",
    "   * $\\theta_1, \\theta_2 ..., \\theta_n$ are the feature weights.   \n",
    "    \n",
    "For people familiar with statistics, the **bias term** is the **intercept** and the **feature weights** are the **betas**. It is just a matter of nomenclature.\n",
    "\n",
    "### <u>Vectorized form</u>\n",
    "The equation can be written in a vectorized for:\n",
    "$$\\hat{y} = h_\\theta(x) = \\theta.x$$  \n",
    "  \n",
    "\n",
    "In this equation:\n",
    "   * $\\theta$ is the model's *parameter vector*, containing the bias term $\\theta_0$ and the feature weights $\\theta_1$ to $\\theta_n$.\n",
    "   * $x$ is the instance's *feature vector*, containing $x_0$ to $x_n$, with $x_0$ always equal to 1.\n",
    "   * $\\theta.x$ is the dot product of the vectors $\\theta$ and $x$, which is equal to $\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$.\n",
    "   * $h_\\theta$ is the hypothesis function using the model parameters $\\theta$.\n",
    "   <br/>\n",
    "   \n",
    "## 2. <u>Linear Algebra </u>  \n",
    "* In machine learning vectors are often represented as *column vectors* which are **2D arrays with a single column**.\n",
    "<br/>\n",
    "\n",
    "### <u>Dot product</u>   \n",
    "   \n",
    " The vectorized form of the equation tells us that $\\hat{y}$ is really the dot product between $\\theta$ and $x$.  \n",
    " \n",
    " In linear algebra, the **dot product** is a way of doing *vector-vector* multiplication. In fact, we said that $\\theta$ is the vector that contain all the model's parameters and that $x$ was the vector that contained the values of a specific subject on all the variables/features.\n",
    "   \n",
    " **<u>EX:</u>**  \n",
    "   \n",
    "   \n",
    "   - If we are measuring the effects of age,weight(kg) and height(cm) on IQ, the *features vector* for a particular individual, Jack, would be:  \n",
    "   \n",
    "   $$x_{\\textit{Jack}} = \\begin{bmatrix}1\\\\\\textit{Age}_{\\textit{Jack}}\\\\\\textit{Weight}_{\\textit{Jack}}\\\\\\textit{Height}_{\\textit{Jack}}\\end{bmatrix} = \\begin{bmatrix}1\\\\22\\\\72\\\\178\\end{bmatrix}$$   \n",
    "<br/>\n",
    "   - And the *parameter vector* would stay the same across all participants(we will see later how to get the parameters):\n",
    "   <br/>\n",
    "   $$\\theta = \\begin{bmatrix}\\theta_0\\\\ \\theta_1 \\\\ \\theta_2\\\\\\theta_3\\end{bmatrix}$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "The result of a *dot product* is always a scalar i.e a single number (which is our case, is $\\hat{y}$).\n",
    "### <u>Compute the dot product</u>\n",
    "\n",
    "\n",
    "- We write the *dot product* using this form: $a.b$\n",
    "- Both vectors must have the same dimensionality.\n",
    "- The calculation is simple: **element 1 of first vector** $\\times$ **element 1 of second vector** $+$ **element 2 of first vector** $\\times$ **element 2 of second vector** etc...\n",
    "- **<u>EX:</u>** $$\\begin{bmatrix}3&2&5\\end{bmatrix}\\bullet\\begin{bmatrix}2&1&1\\end{bmatrix} = 3\\cdot2+2\\cdot1+5\\cdot1 = 6+2+5 = 13 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-dodge",
   "metadata": {},
   "source": [
    "### <u>Compute the dot product in python</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sought-shareware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "v1 = np.array([3,2,5])\n",
    "v2 = np.array([2,1,1])\n",
    "\n",
    "np.dot(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-fundamental",
   "metadata": {},
   "source": [
    "### <u>Dot product in machine learning<u/>:\n",
    "- In machine learning, we like to consider *vectors* as 2D-arrays (matrices) with 1 column and 0 rows. Why? Because our framework rarely needs *vectors*. In fact we are not interested in calculating only one $\\hat{y}$ but rather a group of them. In other terms, we don't want to predict only Jack's score, but also Jane's, Alice's and John's. All at the same time.Therefore the previous Jack's feature vector will become a features matrix with rows = number of subjects and columns = number of features.\n",
    "- That's why we will be using matrix-matrix or matrix-vector multiplication most of the time.\n",
    "- It is therefore conveniant to write the **dot product** formula as a matrix multiplication.\n",
    "- If A and B are two matrices, the matrix multiplication is: $$A^{T}\\cdot B$$\n",
    "- $A^T$ is the *transpose* of A.\n",
    "#### In sum:\n",
    "    - In machine learning we use matrices most of the time (2D vectors).\n",
    "    - Therefore we will define a vector as a 1 column 0 rows matrix.\n",
    "    - So the *dot product* is now conceptualized as a matrix multiplication.\n",
    "<br/>\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-disclaimer",
   "metadata": {},
   "source": [
    "### <u>Matrix multiplication:<u/>\n",
    "- Matrix multiplication is a basic operation in linear algebra.\n",
    "- A matrix is a 2D(rows,columns) array.\n",
    "-**<u>EX:</u>**\n",
    "<br/>\n",
    "    $$\\begin{bmatrix}1&3&5\\\\7&2&1\\end{bmatrix}$$\n",
    "    <br/>\n",
    "    \n",
    "- The basic formula for matrix multiplication is \n",
    "    <br/>\n",
    "    \n",
    "    $$A\\cdot B = A^T \\cdot B$$<br/>\n",
    "    \n",
    "    \n",
    "- **<u>EX:</u>**\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py38] *",
   "language": "python",
   "name": "conda-env-.conda-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
